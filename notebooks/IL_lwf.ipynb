{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "colab": {
   "name": "IL_lwf.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "tvETQMX1ipNf",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1Odgmkpmjsp2tkXXJ3Yoxyukvr2QaEAMA\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab Account AI\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5-LoM_h1IXAi",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "94597526-3fc3-48d1-d339-25af5e98a208"
   },
   "source": [
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "gpu = GPUs[0]\n",
    "print(gpu.name)"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
      "Tesla P100-PCIE-16GB\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "wwN82ZV7ipNg",
    "colab_type": "text"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "RSnex0bmipNh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "DATASET_ROOT = 'cifar-100-python'\n",
    "CODE_ROOT = 'libs'\n",
    "import os\n",
    "if not os.path.isdir(DATASET_ROOT):\n",
    "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
    "    !tar -xf 'cifar-100-python.tar.gz'  \n",
    "    !rm -rf 'cifar-100-python.tar.gz'\n",
    "\n",
    "if not os.path.isdir(CODE_ROOT):\n",
    "  !git clone https://lore-lml:29f601e814e0446c5b17a9f6c3684d1cbd316bcf@github.com/lore-lml/machine-learning2020-incremental_learning.git\n",
    "  !mv 'machine-learning2020-incremental_learning/libs' '.'\n",
    "  !rm -rf 'machine-learning2020-incremental_learning'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import libs.utils as utils\n",
    "from libs.utils import get_one_hot\n",
    "\n",
    "from libs.models.lwf import LwfModel\n",
    "\n",
    "%matplotlib inline"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "7W9y67yoipNk",
    "colab_type": "text"
   },
   "source": [
    "**SET ARGUMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "r0hjWAP3ipNk",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\n",
    "\n",
    "arguments = utils.get_arguments()\n",
    "\n",
    "DEVICE = arguments['DEVICE']\n",
    "NUM_CLASSES = arguments[\"NUM_CLASSES\"] \n",
    "\n",
    "BATCH_SIZE = arguments[\"BATCH_SIZE\"]        # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                                            # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = arguments[\"LR\"]                        # The initial Learning Rate\n",
    "MOMENTUM = arguments[\"MOMENTUM\"]            # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = arguments[\"WEIGHT_DECAY\"]    # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 5#arguments[\"NUM_EPOCHS\"]        # Total number of training epochs (iterations over dataset)\n",
    "GAMMA = arguments[\"GAMMA\"]                  # Multiplicative factor for learning rate step-down\n",
    "\n",
    "LOG_FREQUENCY = arguments[\"LOG_FREQUENCY\"]\n",
    "MILESTONES = arguments[\"MILESTONES\"]\n",
    "SEED = 1993 #arguments[\"SEED\"]\n",
    "\n",
    "OUTPUT_PATH = f\"RUN1_LwF_seed{SEED}\""
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "SaT8eFDNipNm",
    "colab_type": "text"
   },
   "source": [
    "**Define Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "m-ydAGw4ipNm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_transforms, eval_transforms = utils.get_train_eval_transforms()"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "X7Naz_DdipNp",
    "colab_type": "text"
   },
   "source": [
    "**Prepare Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "G-Xct5sNipNp",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "89cd86de-926d-41a9-c0af-30a1ebc3a49c"
   },
   "source": [
    "train_val_dataset = utils.get_cifar_with_seed(DATASET_ROOT, train_transforms, src='train', seed=SEED)\n",
    "test_dataset = utils.get_cifar_with_seed(DATASET_ROOT, eval_transforms, src='test', seed=SEED)\n",
    "\n",
    "print(f\"Size Training Set: {len(train_val_dataset)}\")\n",
    "print(f\"Size Test Set: {len(test_dataset)}\")"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Size Training Set: 50000\n",
      "Size Test Set: 10000\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "xZDP5yXBipNt",
    "colab_type": "text"
   },
   "source": [
    "**Train, Test, Validation functions**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "secPALBtipNt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train_batch(net: LwfModel, train_loader, optimizer, current_step, device=DEVICE):\n",
    "    net.train()\n",
    "    cumulative_loss =.0\n",
    "    running_corrects = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "        \n",
    "        loss = net.compute_distillation_loss(images, labels, outputs, DEVICE)\n",
    "        cumulative_loss += loss.item()\n",
    "        \n",
    "        if current_step != 0 and current_step % LOG_FREQUENCY == 0:\n",
    "                print('\\t\\tTrain step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_step += 1\n",
    "\n",
    "    return cumulative_loss / len(train_loader), running_corrects, current_step\n",
    "\n",
    "def test(net, test_loader, device=DEVICE):\n",
    "    \n",
    "    # confusion matrix\n",
    "    y_true = []\n",
    "    y_preds = []\n",
    "\n",
    "    running_corrects = 0\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        net.eval()\n",
    "        outputs = net(images)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "        # confusion matrix\n",
    "        y_true.extend(labels.data.tolist())\n",
    "        y_preds.extend(preds.tolist())\n",
    "\n",
    "   \n",
    "    return running_corrects, y_true, y_preds\n"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "s5SroLpaipNw",
    "colab_type": "text"
   },
   "source": [
    "**FINE TUNING FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "clnGi_eLipNw",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def lwf_training(train_dataset, test_dataset, max_epoch=NUM_EPOCHS, file_path=OUTPUT_PATH, device=DEVICE):\n",
    "    import math, time\n",
    "    incremental_test = []\n",
    "    train_mean_stage_accuracies = []\n",
    "    test_stage_accuracies = []\n",
    "    cudnn.benchmark\n",
    "    net = LwfModel(100)\n",
    "    criterion = utils.get_criterion('bce')\n",
    "    start_time = time.time()\n",
    "    for stage in range(10):\n",
    "        optimizer, scheduler = utils.get_otpmizer_scheduler(net.parameters(), LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA)\n",
    "        print(f\"STARTING FINE TUNING STAGE {stage+1}...\")\n",
    "        # Get indices\n",
    "        train_idx, test_idx = utils.get_idxs_per_class_of_kth_batch(train_dataset, test_dataset, stage)\n",
    "        \n",
    "        # Make test set incremental\n",
    "        incremental_test.extend(np.ravel(test_idx))\n",
    "        train_idx = np.ravel(train_idx)\n",
    "        train_set, test_set = Subset(train_val_dataset, train_idx), Subset(test_dataset, incremental_test)\n",
    "\n",
    "        # Build data loaders\n",
    "        curr_train_loader = utils.get_train_loader(train_set,batch_size=BATCH_SIZE)\n",
    "        curr_test_loader = utils.get_eval_loader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Init results\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        current_step = 0\n",
    "        \n",
    "        net.before_train(DEVICE)\n",
    "        for epoch in range(max_epoch):\n",
    "            print(f\"\\tSTARTING EPOCH {epoch+1} - LR={scheduler.get_last_lr()}...\")\n",
    "            curr_result = train_batch(net, curr_train_loader, optimizer, current_step, device)\n",
    "            curr_train_loss = curr_result[0]\n",
    "            curr_train_accuracy = curr_result[1] / float(BATCH_SIZE * len(curr_train_loader))\n",
    "            current_step = curr_result[2]\n",
    "            \n",
    "            train_losses.append(curr_train_loss)\n",
    "            train_accuracies.append(curr_train_accuracy)\n",
    "            scheduler.step()\n",
    "            \n",
    "            print(f\"\\t\\tRESULT EPOCH {epoch+1}:\")\n",
    "            print(f\"\\t\\t\\tTrain Loss: {curr_train_loss} - Train Accuracy: {curr_train_accuracy}\\n\")\n",
    "            \n",
    "        \n",
    "        net.after_train(10)\n",
    "        corrects, y_true, y_preds = test(net, curr_test_loader, device)\n",
    "        epoch_test_accuracy = corrects / float(len(test_set))\n",
    "        test_stage_accuracies.append(epoch_test_accuracy)\n",
    "        train_mean_stage_accuracies.append(np.mean(train_accuracies))\n",
    "        \n",
    "        print(f\"\\n\\tResults STAGE {stage+1}:\")\n",
    "        print(f\"\\t\\tTrain Mean Accuracy: {train_mean_stage_accuracies[stage]}\")\n",
    "        print(f\"\\t\\tTest Accuracy: {test_stage_accuracies[stage]}\\n\")\n",
    "\n",
    "\n",
    "    total_time = int(time.time() - start_time)\n",
    "    min = int(total_time / 60)\n",
    "    sec = total_time % 60\n",
    "    print(f\"\\nTotal time: {min} min {sec} sec\\n\")\n",
    "        \n",
    "    return train_mean_stage_accuracies,\\\n",
    "           test_stage_accuracies,\\\n",
    "           y_true, y_preds"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "bvaYg8SiipNy",
    "colab_type": "text"
   },
   "source": [
    "**LEARNING WITHOUT FORGETTING START**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "i_ejvvl4ipNy",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "3e34f5dd-ea20-4526-923a-731e16ef6e81"
   },
   "source": [
    "train_accuracies,\\\n",
    "test_accuracies,\\\n",
    "y_true, y_preds = lwf_training(train_val_dataset, test_dataset, NUM_EPOCHS)"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "STARTING FINE TUNING STAGE 1...\n",
      "\tSTARTING EPOCH 1 - LR=[2]...\n",
      "\t\tTrain step - Step 30, Loss 0.030524257570505142\n",
      "\t\tRESULT EPOCH 1:\n",
      "\t\t\tTrain Loss: 0.06384246958754002 - Train Accuracy: 0.14983974358974358\n",
      "\n",
      "\tSTARTING EPOCH 2 - LR=[2]...\n",
      "\t\tTrain step - Step 60, Loss 0.026574846357107162\n",
      "\t\tRESULT EPOCH 2:\n",
      "\t\t\tTrain Loss: 0.026567643890396144 - Train Accuracy: 0.359375\n",
      "\n",
      "\tSTARTING EPOCH 3 - LR=[2]...\n",
      "\t\tTrain step - Step 90, Loss 0.022238364443182945\n",
      "\t\tRESULT EPOCH 3:\n",
      "\t\t\tTrain Loss: 0.022823121733008288 - Train Accuracy: 0.48217147435897434\n",
      "\n",
      "\tSTARTING EPOCH 4 - LR=[2]...\n",
      "\t\tTrain step - Step 120, Loss 0.021935181692242622\n",
      "\t\tTrain step - Step 150, Loss 0.020367871969938278\n",
      "\t\tRESULT EPOCH 4:\n",
      "\t\t\tTrain Loss: 0.02091297349677636 - Train Accuracy: 0.5314503205128205\n",
      "\n",
      "\tSTARTING EPOCH 5 - LR=[2]...\n",
      "\t\tTrain step - Step 180, Loss 0.018245717510581017\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/8 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\t\tRESULT EPOCH 5:\n",
      "\t\t\tTrain Loss: 0.019858395346464254 - Train Accuracy: 0.5548878205128205\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 14.66it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\tResults STAGE 1:\n",
      "\t\tTrain Mean Accuracy: 0.4155448717948717\n",
      "\t\tTest Accuracy: 0.549\n",
      "\n",
      "STARTING FINE TUNING STAGE 2...\n",
      "\tSTARTING EPOCH 1 - LR=[2]...\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\t\tTrain step - Step 30, Loss 0.0599280521273613\n",
      "\t\tRESULT EPOCH 1:\n",
      "\t\t\tTrain Loss: 0.09131512485253505 - Train Accuracy: 0.07592147435897435\n",
      "\n",
      "\tSTARTING EPOCH 2 - LR=[2]...\n",
      "\t\tTrain step - Step 60, Loss 0.053290966898202896\n",
      "\t\tRESULT EPOCH 2:\n",
      "\t\t\tTrain Loss: 0.054230462186611615 - Train Accuracy: 0.16025641025641027\n",
      "\n",
      "\tSTARTING EPOCH 3 - LR=[2]...\n",
      "\t\tTrain step - Step 90, Loss 0.05080607905983925\n",
      "\t\tRESULT EPOCH 3:\n",
      "\t\t\tTrain Loss: 0.050677869659967914 - Train Accuracy: 0.21494391025641027\n",
      "\n",
      "\tSTARTING EPOCH 4 - LR=[2]...\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-52af08020800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_accuracies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlwf_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_val_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-cd5e13152a60>\u001b[0m in \u001b[0;36mlwf_training\u001b[0;34m(train_dataset, test_dataset, max_epoch, file_path, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\tSTARTING EPOCH {epoch+1} - LR={scheduler.get_last_lr()}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mcurr_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mcurr_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mcurr_train_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-c3d75d1fafb2>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(net, train_loader, optimizer, current_step, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcumulative_loss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ciGlvEWabcbD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import libs.plots as plots\n",
    "\n",
    "method = \"lwf\"\n",
    "plots.plot_accuracy_trend(test_accuracies, method, SEED)\n",
    "plots.plot_confusion_matrix(y_true, y_preds, method, SEED)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "amC8Qy-yVw8W",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def save_accuracies(train_accuracies, val_accuracies, test_accuracies, output=OUTPUT_PATH):\n",
    "  with open(f\"{output}_accuracies.csv\", \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(\"mean_train_acc,test_acc\\n\")\n",
    "    for train, val, test in zip(train_accuracies, val_accuracies, test_accuracies):\n",
    "      f.write(f\"{train},{test}\\n\")\n",
    "    print(\"********** FILE SAVED **********\")\n",
    "\n",
    "\n",
    "save_accuracies(train_accuracies, val_accuracies, test_accuracies)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}